{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building High Performance Data Pipelines with tf.Data and Google Cloud Storage\n",
    "\n",
    "This article presents some recipes on how to build a high performance input pipeline using Tensorflow, specifically tf.data, and Google Cloud Storage.\n",
    "The concepts and techniques are evolved from the slower technique to the fastest, considering the throughput rate from Google Cloud Storage to the training VM.\n",
    "\n",
    "This article uses the Stanford Dogs Dataset with ~20000 images and 120 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark function\n",
    "\n",
    "The benchmark will be the number of images ingested (read) per second from Cloud Storage to the virtual machine. There are several ways to implement this calculation, but a simple function was used to iterate through the dataset and measure the time.\n",
    "\n",
    "The following code snippet ('timeit' function) from Tensorflow documentation [1] (as of 03/18/2020 - version 2.1) is used. Since tf.data.Dataset implements __iter__, it is possible to iterate on this data to observe the progression.\n",
    "\n",
    "\n",
    "[1] https://www.tensorflow.org/tutorials/load_data/images#performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark function for dataset\n",
    "import time\n",
    "default_timeit_steps = 1000\n",
    "\n",
    "def timeit(ds, steps=default_timeit_steps):\n",
    "    start = time.time()\n",
    "    it = iter(ds)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        batch = next(it)\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print('.',end='')\n",
    "    print()\n",
    "    end = time.time()\n",
    "\n",
    "    duration = end-start\n",
    "    print(\"{} batches: {} s\".format(steps, duration))\n",
    "    print(\"{:0.5f} Images/s\".format(BATCH_SIZE*steps/duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the Dataset using tf.data\n",
    "\n",
    "In this case, all the data is located in a bucket from Cloud Storage (gs://).\n",
    "\n",
    "From the path of objects the label is infered and the images downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's import Tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now import some additional libraries\n",
    "from numpy import zeros\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "FILENAMES = 'gs://tf-data-pipeline/*/*'\n",
    "FOLDERS = 'gs://tf-data-pipeline/*'\n",
    "\n",
    "RESOLUTION = (224,224)\n",
    "NUM_TOTAL_IMAGES = 20583\n",
    "IMG_SHAPE=(224,224,3)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels from folders name\n",
    "def get_label_map(path):\n",
    "    #list folders in this path\n",
    "    folders_name = tf.io.gfile.glob(path)\n",
    "\n",
    "    labels = []\n",
    "    for folder in folders_name:\n",
    "        labels.append(folder.split(sep='/')[-1])\n",
    "\n",
    "    # Generate a Label Map\n",
    "    label_map = {labels[i]:i for i in range(len(labels))}\n",
    "    inv_label_map = {i:labels[i] for i in range(len(labels))}\n",
    "    \n",
    "    return label_map, inv_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to One hot encode the inputs\n",
    "def one_hot_encode(label_map, filepath):\n",
    "    dataset = dict()\n",
    "    \n",
    "    for i in range(len(filepath)):\n",
    "        encoding = zeros(len(label_map), dtype='uint8')\n",
    "        encoding[label_map[filepath[i].split(sep='/')[-2]]] = 1\n",
    "        \n",
    "        dataset.update({filepath[i]:list(encoding)})\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map, inv_label_map = get_label_map(FOLDERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in bucket\n",
    "filepath = tf.io.gfile.glob(FILENAMES)\n",
    "NUM_TOTAL_IMAGES = len(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = one_hot_encode(label_map, filepath)\n",
    "dataset = [[k,v] for k,v in dataset.items()]\n",
    "\n",
    "features = [i[0] for i in dataset]\n",
    "labels = [i[1] for i in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset from Features and Labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download bytes from Cloud Storage\n",
    "def get_bytes_label(filepath, label):\n",
    "    raw_bytes = tf.io.read_file(filepath)\n",
    "    return raw_bytes, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Image\n",
    "def process_image(raw_bytes, label):\n",
    "    image = tf.io.decode_jpeg(raw_bytes, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.image.resize(image, (224,224))\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset, batch_size=BATCH_SIZE, cache=False):\n",
    "    \n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.shuffle(NUM_TOTAL_IMAGES)\n",
    "    \n",
    "    # Extraction: IO Intensive\n",
    "    dataset = dataset.map(get_bytes_label, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # Transformation: CPU Intensive\n",
    "    dataset = dataset.map(process_image, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    \n",
    "    # Pipeline next iteration\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = build_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attemp: No cache, no tricks\n",
    "\n",
    "In this first attemp no cache was used and the images were read one by one from the bucket.\n",
    "\n",
    "The biggest problem here is to read 1000's of files one by one. This can really slow down the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(train_ds, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, let's put some local cache in action\n",
    "\n",
    "Tf.data.Dataset implements a cache function. \n",
    "\n",
    "If no parameter is passad to the cache, it uses the memory of the host to cache all the data. The problem is if your dataset is bigger than your host memory and you can't cache the Epoch in memory. In this case the cache won't help and we still have a bottleneck.\n",
    "\n",
    "First let's test the throughput using cache in memory and than in as a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory\n",
    "train_cache_ds = build_dataset(dataset, cache=True)\n",
    "timeit(train_cache_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_local_cache_ds = build_dataset(dataset, cache='./dog.tfcache')\n",
    "timeit(train_local_cache_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hum ...\n",
    "\n",
    "Ok, but no difference?\n",
    "\n",
    "In this case, even using memory and local cache, the host VM is not able to fetch more data, mailly because of the amount of small files.\n",
    "\n",
    "To solve this problem we can follow some best practices for designing performant TensorFlow input pipelines (from the Tensorflow documentation [1]):\n",
    "\n",
    " - Use the prefetch transformation to overlap the work of a producer and consumer.\n",
    " - Parallelize the data reading transformation using the interleave transformation.\n",
    " - Parallelize the map transformation by setting the num_parallel_calls argument.\n",
    " - Use the cache transformation to cache data in memory during the first epoch\n",
    " - Vectorize user-defined functions passed in to the map transformation\n",
    " - Reduce memory usage when applying the interleave, prefetch, and shuffle transformations.\n",
    " \n",
    "But before we continue, let's do some tracing to understand what is going on.\n",
    "\n",
    "[1] https://www.tensorflow.org/guide/data_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.trace_off()\n",
    "tf.summary.trace_on(graph=False, profiler=True)\n",
    "\n",
    "train_ds = build_dataset(dataset)\n",
    "timeit(train_ds, steps=100)\n",
    "\n",
    "tf.summary.trace_export('Data Pipeline', profiler_outdir='/home/jupyter/tensorflow-data-pipeline/logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=/home/jupyter/tensorflow-data-pipeline/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph we can infer that the number of threads created is equal to the number of CPUs of my host VM.\n",
    "\n",
    "Even increasing the 'num_parallel_calls' the performance is the same.\n",
    "\n",
    "The next step is to bundle together all the images in a TFRecord file, so let's do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF.Record for speedup de reading process\n",
    "\n",
    "This test covers 2 parts:\n",
    " - Store images without pre-processing in TFRecord\n",
    " - Store images with cached pre-processing in TFRecord\n",
    " \n",
    "Helper functions to convert from values (float, int, etc.) to tf.train.features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(image, label):\n",
    "    \n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _float_feature(value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=value[0]))    \n",
    "    \n",
    "    def serialize_example(image, label):\n",
    "        \n",
    "        feature = {\n",
    "            'image': _bytes_feature(tf.io.serialize_tensor(image)),\n",
    "            'label': _int64_feature(label)\n",
    "        }\n",
    "\n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "        return example_proto.SerializeToString()\n",
    "    \n",
    "#     tf_string = tf.py_function(\n",
    "#         serialize_example,\n",
    "#         (image, label),  # pass these args to the above function.\n",
    "#         tf.string)      # the return type is `tf.string`.\n",
    "\n",
    "    tf_string = serialize_example(image, label)\n",
    "\n",
    "    return tf_string #tf.reshape(tf_string, ()) # The result is a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True)\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in converted code:\n\n    <ipython-input-74-e3046c9900fe>:6 _bytes_feature  *\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    /tmp/tmpaj7i72pb.py:65 serialize_example\n        feature = {'image': ag__.converted_call(_bytes_feature, (ag__.converted_call(tf.io.serialize_tensor, (image,), None, fscope_4),), None, fscope_4), 'label': ag__.converted_call(_int64_feature, (label,), None, fscope_4)}\n    /tmp/tmpaj7i72pb.py:32 _bytes_feature\n        value = ag__.if_stmt(cond, if_true, if_false, get_state, set_state, ('value',), ())\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:920 if_stmt\n        return _py_if_stmt(cond, body, orelse)\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:1029 _py_if_stmt\n        return body() if cond else orelse()\n    /tmp/tmpaj7i72pb.py:26 if_true\n        value_1 = ag__.converted_call(value_1.numpy, (), None, fscope_1)\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-1bd531b451d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mserialized_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_serialize_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1586\u001b[0m     \"\"\"\n\u001b[1;32m   1587\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3886\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3888\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3889\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   3890\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3139\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in converted code:\n\n    <ipython-input-74-e3046c9900fe>:6 _bytes_feature  *\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    /tmp/tmpaj7i72pb.py:65 serialize_example\n        feature = {'image': ag__.converted_call(_bytes_feature, (ag__.converted_call(tf.io.serialize_tensor, (image,), None, fscope_4),), None, fscope_4), 'label': ag__.converted_call(_int64_feature, (label,), None, fscope_4)}\n    /tmp/tmpaj7i72pb.py:32 _bytes_feature\n        value = ag__.if_stmt(cond, if_true, if_false, get_state, set_state, ('value',), ())\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:920 if_stmt\n        return _py_if_stmt(cond, body, orelse)\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:1029 _py_if_stmt\n        return body() if cond else orelse()\n    /tmp/tmpaj7i72pb.py:26 if_true\n        value_1 = ag__.converted_call(value_1.numpy, (), None, fscope_1)\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n"
     ]
    }
   ],
   "source": [
    "serialized_ds = train_ds.map(tf_serialize_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFRecord with ~9000\n",
    "def create_tfrecord(ds, n_shards):\n",
    "    shard_range = round(NUM_TOTAL_IMAGES/n_shards)\n",
    "    \n",
    "    for i in range(n_shards):\n",
    "        shard = ds.shard(n_shards, i)\n",
    "        batch = map(lambda x: tf_serialize_example(x[0],x[1]), list(train_ds.shard(n_shards,i).as_numpy_iterator()))\n",
    "        \n",
    "        with tf.io.TFRecordWriter(f'output_file-part-{i}.tfrecord') as writer:\n",
    "            for _ in range(shard_range):\n",
    "                batch = tf_serialize_example(next(it))\n",
    "                writer.write(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "renato = map(lambda x: tf_serialize_example(x[0],x[1]), list(train_ds.take(5).shard(5,1).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b9e45e113c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_tfrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-3adddfb7a6ca>\u001b[0m in \u001b[0;36mcreate_tfrecord\u001b[0;34m(ds, n_shards)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mshard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_shards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_serialize_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'output_file-part-{i}.tfrecord'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "create_tfrecord(train_ds.take(5), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 224, 224, 3), dtype=float32, numpy=\n",
       " array([[[[1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          ...,\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       " \n",
       "         [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          ...,\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       " \n",
       "         [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          ...,\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          ...,\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       " \n",
       "         [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          ...,\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       " \n",
       "         [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          ...,\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "          [1.00000000e+00, 1.00000000e+00, 1.00000000e+00]]],\n",
       " \n",
       " \n",
       "        [[[6.03647590e-01, 6.72257066e-01, 7.01750576e-01],\n",
       "          [5.46096444e-01, 5.72795093e-01, 5.82791388e-01],\n",
       "          [5.42044997e-01, 5.80888510e-01, 5.91771722e-01],\n",
       "          ...,\n",
       "          [5.50815940e-01, 6.01568758e-01, 6.06873393e-01],\n",
       "          [5.49999714e-01, 6.08227968e-01, 6.19992673e-01],\n",
       "          [5.54767907e-01, 5.97905159e-01, 6.15540683e-01]],\n",
       " \n",
       "         [[5.99999845e-01, 6.69293225e-01, 7.00578213e-01],\n",
       "          [5.51504254e-01, 5.91315150e-01, 6.00348771e-01],\n",
       "          [5.48125267e-01, 5.68575025e-01, 5.86046159e-01],\n",
       "          ...,\n",
       "          [5.64445615e-01, 5.99703193e-01, 6.19784474e-01],\n",
       "          [5.42594612e-01, 6.00822866e-01, 6.12973690e-01],\n",
       "          [5.57773054e-01, 6.00910306e-01, 6.19258344e-01]],\n",
       " \n",
       "         [[6.11313283e-01, 6.95028782e-01, 7.28617907e-01],\n",
       "          [5.61258554e-01, 5.87464213e-01, 6.02939606e-01],\n",
       "          [5.59725523e-01, 5.66671908e-01, 5.88021338e-01],\n",
       "          ...,\n",
       "          [5.68089306e-01, 5.97246885e-01, 6.27766132e-01],\n",
       "          [5.48373461e-01, 6.06601715e-01, 6.19940460e-01],\n",
       "          [5.57078123e-01, 6.00215375e-01, 6.21176541e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[5.21527708e-01, 4.77572441e-01, 3.80533248e-01],\n",
       "          [5.41914582e-01, 4.77031797e-01, 3.80075425e-01],\n",
       "          [5.40593863e-01, 5.02017319e-01, 3.89775455e-01],\n",
       "          ...,\n",
       "          [3.79450709e-01, 2.96409786e-01, 1.80684268e-01],\n",
       "          [3.97987306e-01, 3.10311258e-01, 2.13012725e-01],\n",
       "          [3.77461642e-01, 3.00757319e-01, 1.84530661e-01]],\n",
       " \n",
       "         [[4.12795395e-01, 3.46057564e-01, 2.64895111e-01],\n",
       "          [4.56315577e-01, 3.93046409e-01, 3.03394675e-01],\n",
       "          [4.61278856e-01, 4.00210142e-01, 2.97812581e-01],\n",
       "          ...,\n",
       "          [3.61729652e-01, 2.73791999e-01, 1.79991990e-01],\n",
       "          [3.72514486e-01, 2.63059855e-01, 1.79694757e-01],\n",
       "          [3.62745225e-01, 2.62009323e-01, 1.79413050e-01]],\n",
       " \n",
       "         [[5.02229929e-01, 4.45957243e-01, 3.53000373e-01],\n",
       "          [5.19048750e-01, 4.60242301e-01, 3.48513156e-01],\n",
       "          [5.15149474e-01, 4.77627873e-01, 3.71201903e-01],\n",
       "          ...,\n",
       "          [3.76436949e-01, 2.94800192e-01, 1.89140126e-01],\n",
       "          [3.82475972e-01, 2.97112197e-01, 1.90301612e-01],\n",
       "          [3.62337142e-01, 2.75199533e-01, 1.78099096e-01]]],\n",
       " \n",
       " \n",
       "        [[[8.08517694e-01, 7.73223579e-01, 7.14399993e-01],\n",
       "          [8.61040354e-01, 8.25746238e-01, 7.66922712e-01],\n",
       "          [8.74509871e-01, 8.39215755e-01, 7.80392230e-01],\n",
       "          ...,\n",
       "          [6.32074356e-01, 5.53642988e-01, 4.08544928e-01],\n",
       "          [5.83335400e-01, 4.06268626e-01, 2.69452035e-01],\n",
       "          [5.41693747e-01, 3.40135902e-01, 1.95369914e-01]],\n",
       " \n",
       "         [[7.79131770e-01, 7.43837595e-01, 6.85014069e-01],\n",
       "          [8.58823895e-01, 8.23529780e-01, 7.64706254e-01],\n",
       "          [8.66894305e-01, 8.31600189e-01, 7.72776663e-01],\n",
       "          ...,\n",
       "          [6.98795438e-01, 6.20364070e-01, 4.75266039e-01],\n",
       "          [7.00179160e-01, 5.46467781e-01, 3.67747366e-01],\n",
       "          [6.22382998e-01, 4.42172557e-01, 2.48634666e-01]],\n",
       " \n",
       "         [[8.52946460e-01, 8.17652345e-01, 7.58828819e-01],\n",
       "          [8.83422613e-01, 8.48128498e-01, 7.89304972e-01],\n",
       "          [8.46349955e-01, 8.11055839e-01, 7.52232254e-01],\n",
       "          ...,\n",
       "          [6.81605995e-01, 6.03174627e-01, 4.63451236e-01],\n",
       "          [7.57202268e-01, 6.20880306e-01, 4.84577686e-01],\n",
       "          [7.06342340e-01, 5.52981198e-01, 4.09142345e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[7.92693019e-01, 7.26734042e-01, 6.61994100e-01],\n",
       "          [7.94819593e-01, 7.39772081e-01, 6.64579272e-01],\n",
       "          [7.87155151e-01, 7.45470762e-01, 6.46197140e-01],\n",
       "          ...,\n",
       "          [5.96507192e-01, 5.15706360e-01, 4.18172091e-01],\n",
       "          [6.41057491e-01, 5.48811257e-01, 4.23198044e-01],\n",
       "          [6.96247041e-01, 5.99442184e-01, 4.71920401e-01]],\n",
       " \n",
       "         [[8.05183947e-01, 7.30139315e-01, 6.48522794e-01],\n",
       "          [7.78159499e-01, 7.07688987e-01, 6.14899457e-01],\n",
       "          [7.71463692e-01, 7.16649055e-01, 6.10187113e-01],\n",
       "          ...,\n",
       "          [6.17022336e-01, 5.22995591e-01, 4.28244233e-01],\n",
       "          [6.14602923e-01, 5.23774505e-01, 3.61503541e-01],\n",
       "          [6.87867582e-01, 5.93749940e-01, 4.21462983e-01]],\n",
       " \n",
       "         [[7.82377899e-01, 6.89905465e-01, 6.00120723e-01],\n",
       "          [7.16136217e-01, 6.33519948e-01, 5.31866074e-01],\n",
       "          [7.60939121e-01, 6.89072669e-01, 5.73438764e-01],\n",
       "          ...,\n",
       "          [6.55931175e-01, 5.48770666e-01, 4.60544229e-01],\n",
       "          [6.01636529e-01, 5.04192591e-01, 3.39241356e-01],\n",
       "          [6.66079938e-01, 5.75069606e-01, 3.78623307e-01]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[2.54901975e-01, 2.58823544e-01, 1.96078449e-01],\n",
       "          [2.54901975e-01, 2.58823544e-01, 1.96078449e-01],\n",
       "          [2.63300806e-01, 2.67222375e-01, 2.04477280e-01],\n",
       "          ...,\n",
       "          [2.50739872e-01, 2.58583009e-01, 2.07602605e-01],\n",
       "          [2.54306704e-01, 2.62149841e-01, 2.07247883e-01],\n",
       "          [2.42559478e-01, 2.50402629e-01, 1.95500657e-01]],\n",
       " \n",
       "         [[2.52118349e-01, 2.56039917e-01, 1.93294838e-01],\n",
       "          [2.54479468e-01, 2.58401036e-01, 1.95655927e-01],\n",
       "          [2.63060242e-01, 2.66981810e-01, 2.04236716e-01],\n",
       "          ...,\n",
       "          [2.50980407e-01, 2.58823544e-01, 2.07843155e-01],\n",
       "          [2.54306704e-01, 2.62149841e-01, 2.07247883e-01],\n",
       "          [2.45553181e-01, 2.53396332e-01, 1.98494360e-01]],\n",
       " \n",
       "         [[2.48288855e-01, 2.52210438e-01, 1.89465329e-01],\n",
       "          [2.49776334e-01, 2.53697902e-01, 1.90952808e-01],\n",
       "          [2.58420885e-01, 2.62342453e-01, 1.99597359e-01],\n",
       "          ...,\n",
       "          [2.47689053e-01, 2.55532205e-01, 2.04551801e-01],\n",
       "          [2.53588915e-01, 2.61432052e-01, 2.06530094e-01],\n",
       "          [2.45553181e-01, 2.53396332e-01, 1.98494360e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.69579715e-01, 1.58725381e-01, 1.13172159e-01],\n",
       "          [2.24294618e-01, 2.23104134e-01, 1.76045313e-01],\n",
       "          [2.27202266e-01, 2.35675663e-01, 1.84380144e-01],\n",
       "          ...,\n",
       "          [1.44351348e-01, 1.41147599e-01, 1.19386286e-01],\n",
       "          [1.54372707e-01, 1.51168957e-01, 1.29407644e-01],\n",
       "          [1.44052207e-01, 1.40848458e-01, 1.19087152e-01]],\n",
       " \n",
       "         [[2.04286084e-01, 1.93431750e-01, 1.47878528e-01],\n",
       "          [2.43968874e-01, 2.42778391e-01, 1.95719570e-01],\n",
       "          [2.11845696e-01, 2.20319092e-01, 1.69023558e-01],\n",
       "          ...,\n",
       "          [1.37558043e-01, 1.33636475e-01, 1.15166530e-01],\n",
       "          [9.40163434e-02, 9.00947750e-02, 7.16248155e-02],\n",
       "          [1.18268825e-01, 1.14347257e-01, 9.58772972e-02]],\n",
       " \n",
       "         [[2.22893283e-01, 2.12038949e-01, 1.66485712e-01],\n",
       "          [2.29075298e-01, 2.27884829e-01, 1.80825993e-01],\n",
       "          [2.45756760e-01, 2.54230142e-01, 2.02934638e-01],\n",
       "          ...,\n",
       "          [1.56613350e-01, 1.52691782e-01, 1.37005508e-01],\n",
       "          [1.32142797e-01, 1.28221229e-01, 1.12534955e-01],\n",
       "          [1.17647067e-01, 1.13725498e-01, 9.80392247e-02]]],\n",
       " \n",
       " \n",
       "        [[[6.32664979e-01, 6.99331641e-01, 7.62076795e-01],\n",
       "          [6.16351545e-01, 6.83018208e-01, 7.45763302e-01],\n",
       "          [6.20133042e-01, 6.86799705e-01, 7.49544859e-01],\n",
       "          ...,\n",
       "          [6.22723997e-01, 6.77625954e-01, 7.12920070e-01],\n",
       "          [6.33508325e-01, 6.88410282e-01, 7.23704398e-01],\n",
       "          [6.29233539e-01, 6.84135497e-01, 7.19429612e-01]],\n",
       " \n",
       "         [[6.19844854e-01, 6.88139677e-01, 7.47628450e-01],\n",
       "          [6.05847359e-01, 6.74142182e-01, 7.33630955e-01],\n",
       "          [6.11198843e-01, 6.79493666e-01, 7.38982499e-01],\n",
       "          ...,\n",
       "          [5.98056674e-01, 6.52958632e-01, 6.88252747e-01],\n",
       "          [6.08841002e-01, 6.63742959e-01, 6.99037075e-01],\n",
       "          [6.03238821e-01, 6.58140779e-01, 6.93434894e-01]],\n",
       " \n",
       "         [[6.18645012e-01, 6.89233243e-01, 7.44135201e-01],\n",
       "          [6.09238088e-01, 6.79826319e-01, 7.34728277e-01],\n",
       "          [6.19030118e-01, 6.89618349e-01, 7.44520307e-01],\n",
       "          ...,\n",
       "          [6.78755820e-01, 7.33657837e-01, 7.68951952e-01],\n",
       "          [6.89927876e-01, 7.44829834e-01, 7.80123949e-01],\n",
       "          [6.84907079e-01, 7.39809096e-01, 7.75103211e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[4.79312301e-01, 4.24410343e-01, 3.18527967e-01],\n",
       "          [4.83115703e-01, 4.28213745e-01, 3.22331369e-01],\n",
       "          [4.69249815e-01, 4.14347857e-01, 3.08465481e-01],\n",
       "          ...,\n",
       "          [5.16791642e-01, 4.61889714e-01, 3.56007338e-01],\n",
       "          [5.05210936e-01, 4.50308979e-01, 3.44426602e-01],\n",
       "          [4.88083601e-01, 4.33181643e-01, 3.27299267e-01]],\n",
       " \n",
       "         [[4.77973819e-01, 4.23071861e-01, 3.17189485e-01],\n",
       "          [4.89417970e-01, 4.34516013e-01, 3.28633636e-01],\n",
       "          [5.15167594e-01, 4.60265636e-01, 3.54383290e-01],\n",
       "          ...,\n",
       "          [5.29398620e-01, 4.74496692e-01, 3.68614316e-01],\n",
       "          [4.88260955e-01, 4.33358997e-01, 3.27476621e-01],\n",
       "          [4.87984151e-01, 4.33082193e-01, 3.27199817e-01]],\n",
       " \n",
       "         [[4.72326487e-01, 4.17424530e-01, 3.11542153e-01],\n",
       "          [4.88230377e-01, 4.33328420e-01, 3.27446043e-01],\n",
       "          [5.49789906e-01, 4.94887888e-01, 3.89005542e-01],\n",
       "          ...,\n",
       "          [5.28509140e-01, 4.73607212e-01, 3.67724836e-01],\n",
       "          [4.79799986e-01, 4.24898028e-01, 3.19015652e-01],\n",
       "          [5.00114977e-01, 4.45213020e-01, 3.39330643e-01]]],\n",
       " \n",
       " \n",
       "        [[[3.56392731e-04, 8.55592359e-03, 0.00000000e+00],\n",
       "          [2.50725318e-02, 1.42557023e-04, 0.00000000e+00],\n",
       "          [1.39659643e-01, 6.69255406e-02, 7.69533068e-02],\n",
       "          ...,\n",
       "          [7.66945779e-02, 9.23808590e-02, 9.63024274e-02],\n",
       "          [5.70200384e-02, 7.27063119e-02, 7.66278803e-02],\n",
       "          [3.08859191e-04, 1.24061992e-02, 1.63277686e-02]],\n",
       " \n",
       "         [[6.09768890e-02, 2.68932562e-02, 2.29716878e-02],\n",
       "          [1.79114148e-01, 1.23610690e-01, 1.00350142e-01],\n",
       "          [4.36432123e-01, 3.50630313e-01, 2.99072176e-01],\n",
       "          ...,\n",
       "          [5.05312271e-02, 6.62175044e-02, 7.01390728e-02],\n",
       "          [2.69669797e-02, 4.26532552e-02, 4.65748236e-02],\n",
       "          [1.54072847e-02, 3.10935602e-02, 3.50151286e-02]],\n",
       " \n",
       "         [[1.54197931e-01, 7.29129240e-02, 8.11549649e-02],\n",
       "          [2.72152603e-01, 1.79493055e-01, 1.23214297e-01],\n",
       "          [4.45538282e-01, 3.41701716e-01, 2.21641198e-01],\n",
       "          ...,\n",
       "          [4.39545512e-03, 1.78233292e-02, 2.17448976e-02],\n",
       "          [2.95219887e-02, 4.52082604e-02, 4.91298325e-02],\n",
       "          [4.63793911e-02, 6.20656647e-02, 6.59872293e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[2.74128556e-01, 2.90410340e-01, 2.62959361e-01],\n",
       "          [2.15988144e-01, 2.30963275e-01, 2.07900196e-01],\n",
       "          [2.13250369e-01, 2.27869123e-01, 2.06002742e-01],\n",
       "          ...,\n",
       "          [2.66177177e-01, 2.57791191e-01, 1.52030990e-01],\n",
       "          [5.30039549e-01, 4.95323271e-01, 3.58207405e-01],\n",
       "          [6.35443866e-01, 5.66434741e-01, 4.62210774e-01]],\n",
       " \n",
       "         [[2.45623335e-01, 2.39706039e-01, 2.20098197e-01],\n",
       "          [2.59450048e-01, 2.52748728e-01, 2.37006143e-01],\n",
       "          [2.35131606e-01, 2.28216454e-01, 2.13528037e-01],\n",
       "          ...,\n",
       "          [4.44092214e-01, 4.19921339e-01, 3.71344030e-01],\n",
       "          [6.32240117e-01, 5.83500564e-01, 5.25699973e-01],\n",
       "          [4.29917574e-01, 3.46937746e-01, 3.31948251e-01]],\n",
       " \n",
       "         [[2.26786077e-01, 2.07178235e-01, 1.91491961e-01],\n",
       "          [2.22778693e-01, 2.03170851e-01, 1.90565810e-01],\n",
       "          [1.98859617e-01, 1.79251760e-01, 1.67487055e-01],\n",
       "          ...,\n",
       "          [3.66509497e-01, 3.44594479e-01, 2.76887387e-01],\n",
       "          [2.44367942e-01, 1.98780805e-01, 1.32180691e-01],\n",
       "          [1.64539561e-01, 8.21866095e-02, 6.98045045e-02]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(32, 121), dtype=int32, numpy=\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renato = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
